{
  "hash": "c9a166fe6deb5208d1b76def8dd310b8",
  "result": {
    "markdown": "---\nengine: knitr\n---\n\n\n# From Parameter Estimation to Hypothesis Testing: Building a Bayesian A/B Test\n\n> In this chapter, we’ll test our belief that removing an image from an email will increase the [click-through rate](https://en.wikipedia.org/wiki/Click-through_rate) against the belief that removing it will hurt the click-through rate. … \n>\n> Since we already know how to estimate a single unknown parameter, all we need to do for our test is estimate both parameters—that is, the conversion rates of each email. Then we’ll use R to run a Monte Carlo simulation and determine which hypothesis is likely to perform better—in other words, which variant, A or B, is superior. A/B tests can be performed using classical statistical techniques such as <a class='glossary' title='A t-test is a type of statistical analysis used to compare the averages of two groups and determine whether the differences between them are more likely to arise from random chance. (Wikipedia)'>t-tests</a>, but building our test the Bayesian way will help us understand each part of it intuitively and give us more useful results as well.\n\n## Setting Up a Bayesian A/B Test\n\n> For our test we’re going to send one variant with images like usual, and another without images. The test is called an [A/B test](https://en.wikipedia.org/wiki/A/B_testing) because we are comparing variant A (with image) and variant B (without) to determine which one performs better.\n\n> The 300 people we’re going to test will be split up into two groups, A and B. Group A will receive the usual email with a big picture at the top, and group B will receive an email with no picture. The hope is that a simpler email will feel less “spammy” and encourage users to click through to the content.\n\n## Finding Our Prior Probability\n\n> We’ve run an email campaign every week, so from that data we have a reasonable expectation that the probability of clicking the link to the blog on any given email should be around 30 percent. … We’ll settle on Beta(3,7) for our prior probability distribution. This distribution allows us to represent a beta distribution where 0.3 is the mean, but a wide range of possible alternative rates are considered.\n\n![Visualizing our prior probability distribution](img/15fig01.jpg){#fig-15-01 \nfig-alt=\"Beta distribution with modus about 0.25\" fig-align=\"center\" \nwidth=\"70%\"}\n\n### Collecting Data\n\n|           | Clicked | Not clicked | Observed conversion rate |\n|-----------|---------|-------------|--------------------------|\n| Variant A | 36      | 114         | 0.24                     |\n| Variant B | 50      | 100         | 0.33                     |\n\n: Email Click-through Rates {#tbl-click-through-rates}\n\nWe are going to add beta and likelihood probabilities using @eq-add-two-beta:\n\nPrior: Beta(3,7)\nLikelihood Variant A (with picture): Beta(3 + 36, 7 + 114) = Beta(39, 121)\nLikelihood Variant B:(without picture) Beta(3 + 50, 7 + 100) = Beta(53, 107)\n\n![Beta distributions for our estimates for both variants of our email](img/15fig02.jpg){#fig-15-02 \nfig-alt=\"Two distribution peaked at about 0.24 and 0.33\" fig-align=\"center\" \nwidth=\"70%\"}        \n\nVariant B looks better, but there is an overlap. \n\n> how sure can we be that B is the better variant? This is where the Monte Carlo simulation comes in.\n\n## Monte Carlo Simulations\n\n> A <a class='glossary' title='Markov chain Monte Carlo (MCMC) methods comprise a class of algorithms for sampling from a probability distribution. By constructing a MARKOV CHAIN that has the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the chain. The more steps that are included, the more closely the distribution of the sample matches the actual desired distribution. Various algorithms exist for constructing chains. (Wikipedia)'>Monte Carlo simulation</a> is any technique that makes use of random sampling to solve a problem. In this case, we’re going to randomly sample from the two distributions, where each sample is chosen based on its probability in the distribution so that samples in a high-probability region will appear more frequently. \n\n> We can imagine that the posterior distribution represents all the worlds that could exist based on our current state of beliefs regarding each conversion rate.\n\n### In How Many Worlds Is B the Better Variant?\n\n\n::: {.cell}\n\n```{.r .cell-code #lst-MC-manually lst-cap=\"Monte Carlo simulation from scratch\"}\nn.trials = 1e5\nprior.alpha = 3\nprior.beta = 7\n\na.samples <- rbeta(n.trials, 36 + prior.alpha, 114 + prior.beta)\nb.samples <- rbeta(n.trials, 50 + prior.alpha, 100 + prior.beta)\n\np.b_superior <- sum(b.samples > a.samples)/n.trials\n\np.b_superior\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#> [1] 0.95843\n```\n\n\n:::\n:::\n\n> What we see here is that in 96 percent of the 100,000 trials, variant B was superior. We can imagine this as looking at 100,000 possible worlds.\n\n::: {.callout-caution}\nWill Kurt remarks that this calculation was like a single t-test with a flat prior Beta(1,1) resulting in a p-value of 0.4, often considered \"statistically significant\". But it seems that the Monte Carlo simulation has to advantages:\n\n1. We built this test from scratch (as Will argued)\n2. We do not only know how sure we can be that B is the better variant, but also exactly how much better the B variant is (as Will argued in the next section)\n3. The MCMC simulation shows with the posterior distribution all possible worlds, instead of just retaining or rejecting a hypothesis (my additional argument).\n\nI want to look into the details and learn how to do this. There is a wonderful vignette [Tidy t-Test with {**infer**}](https://infer.tidymodels.org/articles/t_test.html) that I could read as a starter.\n:::\n\n### How Much Better Is Each Variant B Than Each Variant A?\n\n> Now we can say precisely how certain we are that B is the superior variant. … We can take the exact results from our last simulation and test how much better variant B is likely to be by looking at how many times greater the B samples are than the A samples.\n\n$$\\frac{\\text{B Samples}}{\\text{A Samples}}$$\n\n> In R, if we take the `a.samples` and `b.samples` from before, we can compute `b.samples/a.samples`. This will give us a distribution of the relative improvements from variant A to variant B. When we plot out this distribution as a histogram, as shown in @fig-15-03, we can see how much we expect variant B to improve our click-through rate.\n\n![A histogram of possible improvements we might see](img/15fig03.jpg){#fig-15-03 \nfig-alt=\"Histogram with modus about 1.4\" fig-align=\"center\" \nwidth=\"70%\"}\n\n> From this histogram we can see that variant B will most likely be about a 40 percent improvement (ratio of 1.4) over A, although there is an entire range of possible values. \n\nAs we discussed in @sec-chap-13, the <a class='glossary' title='A cumulative distribution function (CDF) tells us the probability that a random variable takes on a value less than or equal to x. (Statology) It sums all parts of the distribution, replacing a lot of calculus work. The CDF takes in a value and returns the probability of getting that value or lower. (BF, Chap.13) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (Statistics How To)'>cumulative distribution function</a> (CDF) is much more useful than a histogram for reasoning about our results. Since we’re working with data rather than a mathematical function, we’ll compute the <a class='glossary' title='In statistics, an empirical distribution function (commonly also called an empirical cumulative distribution function, eCDF) is the distribution function associated with the empirical measure of a sample. This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points. Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value. (Wikipedia) A CDF is a hypothetical model of a distribution, the ECDF models empirical (i.e. observed) data. (Statistics How To)'>empirical cumulative distribution function</a> with R’s `ecdf()` function. The eCDF is illustrated in @fig-15-04.\n\n![A distribution of possible improvements we might see](img/15fig04.jpg){#fig-15-04 \nfig-alt=\"The ECDF in form a S-curve with quantiles every 25%.\" fig-align=\"center\" \nwidth=\"70%\"}\n\n::: {.callout-note}\nIn my experiments I will use the `ggplot2::stat_ecdf()` function as demonstrated already in @lst-fig-pb-13-4d.\n:::\n\n> Now we can see our results more clearly. There is really just a small, small chance that A is better, and even if it is better, it’s not going to be by much. We can also see that there’s about a 25 percent chance that variant B is a 50 percent or more improvement over A, and even a reasonable chance it could be more than double the conversion rate! Now, in choosing B over A, we can actually reason about our risk by saying, “The chance that B is 20 percent worse is roughly the same that it’s 100 percent better.” Sounds like a good bet to me, and a much better statement of our knowledge than, “There is a statistically significant difference between B and A.”\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}